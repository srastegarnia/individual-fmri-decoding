{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/SRastegarnia/.virtualenvs/hcptrtr_gcn_env/lib/python3.6/site-packages/nilearn/__init__.py:69: FutureWarning: Python 3.6 support is deprecated and will be removed in release 0.10 of Nilearn. Consider switching to Python 3.8 or 3.9.\n",
      "  _python_deprecation_warnings()\n",
      "/home/SRastegarnia/.virtualenvs/hcptrtr_gcn_env/lib/python3.6/site-packages/nilearn/input_data/__init__.py:27: FutureWarning: The import path 'nilearn.input_data' is deprecated in version 0.9. Importing from 'nilearn.input_data' will be possible at least until release 0.13.0. Please import from 'nilearn.maskers' instead.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nilearn.connectome\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn.maskers import NiftiMasker\n",
    "from nilearn.input_data import NiftiLabelsMasker, NiftiMapsMasker\n",
    "from csv import writer\n",
    "from load_confounds import Params9\n",
    "\n",
    "from dypac.masker import LabelsMasker, MapsMasker\n",
    "from nilearn.plotting import plot_roi, plot_stat_map\n",
    "from nilearn import datasets\n",
    "from nilearn.interfaces.fmriprep import load_confounds_strategy\n",
    "\n",
    "sys.path.append(os.path.join(\"..\"))\n",
    "import time_windows_dataset\n",
    "import graph_construction\n",
    "import gcn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"torch v{}\".format(torch.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"nilearn v{}\".format(nilearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TR = 1.49\n",
    "random_seed = 0\n",
    "\n",
    "subject = 'sub-02'\n",
    "region_approach = 'dypac'\n",
    "resolution = 1024\n",
    "window_length = 1\n",
    "modality = 'all_mod' #'motor'\n",
    "HRFlag_processes = '3volumes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "data_dir = os.path.join('..', '..', '..', 'data')\n",
    "\n",
    "concat_data_dir = os.path.join(data_dir, 'concat_data', region_approach, str(resolution), subject)\n",
    "processed_bold_files = sorted(glob.glob(concat_data_dir + '/*.npy'))\n",
    "\n",
    "conn_dir = os.path.join(data_dir, 'connectomes')\n",
    "conn_files = sorted(glob.glob(conn_dir + '/conn_friends_{}_{}{}.npy'.format(subject,\n",
    "                                                                            region_approach,\n",
    "                                                                            resolution)))\n",
    "\n",
    "split_dir = os.path.join(data_dir, 'split_win_data')\n",
    "out_csv = os.path.join(split_dir, 'labels.csv')\n",
    "out_file = os.path.join(split_dir, '{}_{:04d}.npy')\n",
    "\n",
    "result_dir = os.path.join('../results')\n",
    "result_csv = os.path.join(result_dir, 'result_df.csv')\n",
    "model_path = os.path.join('../models', 'gcn_test.pt')\n",
    "\n",
    "if not os.path.exists(split_dir):\n",
    "    os.makedirs(split_dir)\n",
    "if not os.path.exists(conn_dir):\n",
    "    os.makedirs(conn_dir)    \n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n",
    "if not os.path.exists(result_csv):\n",
    "    result_df = pd.DataFrame(columns=['subject','modality','window_length','region_approach',\n",
    "                                      'average_loss','average_accuracy', 'time_window'])\n",
    "    result_df.to_csv(result_csv, index=False)\n",
    "    \n",
    "# remove previous content\n",
    "if os.path.exists(split_dir):\n",
    "    files = glob.glob(os.path.join(split_dir, \"*\"))\n",
    "    for f in files:\n",
    "        os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating connectomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# فقط یک بار نیاز است که اجرا شود و فایل ذخیره شده\n",
    "# Generates connectome from friends for GCN\n",
    "bold_suffix = '_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz'\n",
    "path_cneuromod = '/data/neuromod/projects/ml_models_tutorial/data/friends/raw_data'\n",
    "file_epi = os.path.join(path_cneuromod, \n",
    "                        '{}_ses-003_task-s01e06a_space-MNI152NLin2009cAsym_desc-' \\\n",
    "                        'preproc_bold.nii.gz'.format(subject))\n",
    "\n",
    "conf = load_confounds_strategy(file_epi, denoise_strategy='simple', global_signal='basic')\n",
    "\n",
    "path_dypac = '/data/cisl/pbellec/models'\n",
    "file_mask = os.path.join(path_dypac, '{}_space-MNI152NLin2009cAsym_label-GM_mask.nii.gz'.format(subject))\n",
    "file_dypac = os.path.join(path_dypac, '{}_space-MNI152NLin2009cAsym_desc-dypac{}_' \\\n",
    "                          'components.nii.gz'.format(subject,resolution))\n",
    "\n",
    "masker = NiftiMasker(standardize=True, detrend=False, smoothing_fwhm=5, mask_img=file_mask)\n",
    "masker.fit(file_epi)\n",
    "\n",
    "maps_masker = MapsMasker(masker=masker, maps_img=file_dypac)\n",
    "\n",
    "sample_ts= maps_masker.transform(img=file_epi, confound=conf[0])\n",
    "sample_ts.shape\n",
    "\n",
    "# Estimating connectomes\n",
    "corr_measure = nilearn.connectome.ConnectivityMeasure(kind=\"correlation\")\n",
    "conn = corr_measure.fit_transform([sample_ts])[0]\n",
    "np.save(os.path.join(conn_dir, 'conn_friends_{}_{}{}.npy'.format(subject, region_approach, resolution)), conn)\n",
    "\n",
    "# # conn_file = os.path.join(conn_dir, 'conn_friends_{}_{}{}.npy'.format(subject, region_approach, resolution))\n",
    "# # print(conn_file)\n",
    "# # a = np.load(conn_file)\n",
    "# # print(np.shape(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split timeseries & generate label file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_labels = {'body0b':0,'body2b':1,'face0b':2,'face2b':3,'fear':4,'footL':5,'footR':6,\n",
    "              'handL':7,'handR':8,'match':9,'math':10,'mental':11,'place0b':12,'place2b':13, \n",
    "              'random':14,'relational':15,'shape':16,'story':17,'tongue':18,'tool0b':19,'tool2b':20}\n",
    "\n",
    "\n",
    "label_df = pd.DataFrame(columns=['label', 'filename'])\n",
    "# print(len(processed_bold_files))\n",
    "\n",
    "for proc_bold in processed_bold_files:\n",
    "    \n",
    "    ts_data = np.load(proc_bold)\n",
    "#     print(ts_data)\n",
    "    \n",
    "    ts_duration = len(ts_data)\n",
    "#     print(ts_duration)\n",
    "    \n",
    "    ts_filename = os.path.basename(proc_bold)\n",
    "    ts_filename = \"\".join(ts_filename.split(\".\")[:-1])\n",
    "    print(ts_filename)\n",
    "    \n",
    "    ts_label = ts_filename.split(subject+'_', 1)[1].split('_'+HRFlag_processes, 1)[0]\n",
    "    print('ts_label:', ts_label)\n",
    "    \n",
    "    valid_label = dic_labels[ts_label]\n",
    "    \n",
    "    # Split the timeseries\n",
    "    rem = ts_duration % window_length\n",
    "    n_splits = int(np.floor(ts_duration / window_length))\n",
    "    print('n_splits:', n_splits)\n",
    "    ts_data = ts_data[:(ts_duration-rem), :]\n",
    "    print('ts_data shape after removing rem:', np.shape(ts_data), '\\n')    \n",
    "    \n",
    "    for j, split_ts in enumerate(np.split(ts_data, n_splits)):\n",
    "        ts_output_file_name = out_file.format(ts_filename, j)\n",
    "        print('ts_output_file_name:', ts_output_file_name)        \n",
    "#         print('shape split_ts:', np.shape(split_ts))\n",
    "#         print('split_ts:', split_ts)\n",
    "\n",
    "        split_ts = np.swapaxes(split_ts, 0, 1)\n",
    "        np.save(ts_output_file_name, split_ts)\n",
    "        curr_label = {'label': valid_label, 'filename': os.path.basename(ts_output_file_name)}\n",
    "        label_df = label_df.append(curr_label, ignore_index=True)\n",
    "    print('------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "label_df.to_csv(out_csv, index=False)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch dataset: generates items from the current data directory\n",
    "train_dataset = time_windows_dataset.TimeWindowsDataset(\n",
    "    data_dir=split_dir\n",
    "    , partition=\"train\"\n",
    "    , random_seed=random_seed\n",
    "    , pin_memory=True\n",
    "    , normalize=True,shuffle = True)\n",
    "valid_dataset = time_windows_dataset.TimeWindowsDataset(\n",
    "    data_dir=split_dir\n",
    "    , partition=\"valid\"\n",
    "    , random_seed=random_seed\n",
    "    , pin_memory=True\n",
    "    , normalize=True, shuffle = True)\n",
    "test_dataset = time_windows_dataset.TimeWindowsDataset(\n",
    "    data_dir=split_dir\n",
    "    , partition=\"test\"\n",
    "    , random_seed=random_seed\n",
    "    , pin_memory=True\n",
    "    , normalize=True, shuffle = True)\n",
    "print(\"train dataset: {}\".format(train_dataset))\n",
    "print(\"valid dataset: {}\".format(valid_dataset))\n",
    "print(\"test dataset: {}\".format(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch dataloader: wraps an iterable around pytorch dataset to shuffle & generate (in parrallel) minibatches\n",
    "#setting pytoch seed for reproducible torch.utils.data.DataLoader\n",
    "torch.manual_seed(random_seed)\n",
    "train_generator = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "valid_generator = torch.utils.data.DataLoader(valid_dataset, batch_size=10, shuffle=True)\n",
    "test_generator = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=True)\n",
    "train_features, train_labels = next(iter(train_generator))\n",
    "print(f\"Feature batch shape: {train_features.size()}; mean {torch.mean(train_features)}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}; mean {torch.mean(torch.Tensor.float(train_labels))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model definition\n",
    "## get average connectome with its k-nearest neighbors\n",
    "# connectomes = np.load(conn_file)\n",
    "\n",
    "connectomes = []\n",
    "for conn_file in conn_files:\n",
    "    print(conn_file)\n",
    "    connectomes += [np.load(conn_file)]\n",
    "\n",
    "\n",
    "# connectomes = RawDataLoad.get_valid_connectomes()\n",
    "graph = graph_construction.make_group_graph(connectomes, k=8, self_loops=False, symmetric=True)\n",
    "\n",
    "## Create model\n",
    "gcn = gcn_model.GCN(graph.edge_index, graph.edge_attr, \n",
    "                    n_timepoints=window_length, resolution=resolution)\n",
    "gcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(connectomes)\n",
    "# print(connectomes.type)\n",
    "print(np.shape(np.load('../../data/connectomes/conn_friends_sub-02_dypac1024.npy')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)    \n",
    "    model = model.double() #shima    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X.double())# shima\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss, current = loss.item(), batch * dataloader.batch_size #Loic\n",
    "\n",
    "        correct = (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        correct /= X.shape[0]\n",
    "        print(f\"#{batch:>5};\\ttrain_loss: {loss:>0.3f};\\ttrain_accuracy:{(100*correct):>5.1f}%\\t\\t[{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def valid_test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model.forward(X)\n",
    "            loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    loss /= size\n",
    "    correct /= size\n",
    "\n",
    "    return loss, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Train and evaluate the model\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(gcn.parameters(), lr=1e-4, weight_decay=5e-4)\n",
    "\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}/{epochs}\\n-------------------------------\")\n",
    "    train_loop(train_generator, gcn, loss_fn, optimizer)\n",
    "    loss, correct = valid_test_loop(valid_generator, gcn, loss_fn)\n",
    "    print(f\"Valid metrics:\\n\\t avg_loss: {loss:>8f};\\t avg_accuracy: {(100*correct):>0.1f}%\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, correct = valid_test_loop(test_generator, gcn, loss_fn) \n",
    "print(f\"Test metrics:\\n\\t avg_loss: {loss:>8f};\\t avg_accuracy: {(100*correct):>0.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_loss = 100*float(\"{:.2f}\".format(loss))\n",
    "correct = 100*float(\"{:.4f}\".format(correct))\n",
    "\n",
    "print(average_loss)\n",
    "print(correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_list_as_row(file_name, list_of_elem):\n",
    "    with open(file_name, 'a+', newline='') as write_obj:\n",
    "        csv_writer = writer(write_obj)\n",
    "        csv_writer.writerow(list_of_elem)\n",
    "        \n",
    "time_window = window_length*1.49         \n",
    "\n",
    "row_contents = [subject, modality, window_length, region_approach, \n",
    "               average_loss, correct, time_window]\n",
    "append_list_as_row(result_csv, row_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = {'subject': subject,'modality': modality,\n",
    "#            'window_length': window_length,'region_approach': region_approach,\n",
    "#            'average_loss': average_loss,'average_accuracy': correct}\n",
    "\n",
    "# result_df = result_df.append(results, ignore_index=True)\n",
    "# result_df.to_csv(result_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X,y in test_generator:\n",
    "    print('X:', X.shape)\n",
    "    print('y:', y.shape)\n",
    "    print(X.mean())\n",
    "    print(y)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gcn.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(result_csv, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple line plots\n",
    "\n",
    "plt.plot( 'time_window', 'average_accuracy', data=df, marker='', \n",
    "         color='#4a996f', linewidth=2, label='Average accuracy')\n",
    "plt.plot( 'time_window', 'average_loss', data=df, marker='', color='#d1cd10', \n",
    "         linewidth=2, linestyle='dashed', label='Average loss')\n",
    "plt.title(subject, 'all 21 conditions')\n",
    "plt.xlabel(\"Window time(sec)\")\n",
    "\n",
    "# show legend\n",
    "plt.legend()\n",
    "\n",
    "# show graph\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "hcptrtr_gcn_env",
   "language": "python",
   "name": "hcptrtr_gcn_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
